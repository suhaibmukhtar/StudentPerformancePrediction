Now that you have your MLOps project structure, let's proceed with deploying the best model using FastAPI and creating a Dockerfile for containerization.

ğŸš€ Steps for Deployment
Ensure your trained model & preprocessor are saved in artifacts/
Build a FastAPI service to load the model and handle predictions.
Write a Dockerfile to containerize the FastAPI service.
Run the FastAPI app locally for testing.
Build and run the Docker container.
1ï¸âƒ£ Save Your Model & Preprocessor
Ensure your best model and preprocessor are saved in the artifacts/ directory inside the training pipeline (train_pipeline.py).

Modify src/pipeline/train_pipeline.py
Add this after training your best model:

import pickle
import os

def save_model_and_preprocessor(model, preprocessor):
    """Saves the trained model and preprocessing pipeline."""
    os.makedirs("artifacts", exist_ok=True)

    # Save model
    with open("artifacts/best_model.pkl", "wb") as f:
        pickle.dump(model, f)

    # Save preprocessor
    with open("artifacts/preprocessor.pkl", "wb") as f:
        pickle.dump(preprocessor, f)

    print("âœ… Model and Preprocessor saved in artifacts/")
2ï¸âƒ£ Create a FastAPI App (src/app.py)
This FastAPI app loads the model & preprocessor and serves predictions.

Create src/app.py

from fastapi import FastAPI
from pydantic import BaseModel
import numpy as np
import pickle
import pandas as pd
import os

# Initialize FastAPI App
app = FastAPI()

# Load Model & Preprocessor
MODEL_PATH = "artifacts/best_model.pkl"
PREPROCESSOR_PATH = "artifacts/preprocessor.pkl"

if os.path.exists(MODEL_PATH) and os.path.exists(PREPROCESSOR_PATH):
    with open(MODEL_PATH, "rb") as model_file:
        model = pickle.load(model_file)
    with open(PREPROCESSOR_PATH, "rb") as preprocessor_file:
        preprocessor = pickle.load(preprocessor_file)
    print("âœ… Model & Preprocessor Loaded Successfully!")
else:
    raise FileNotFoundError("Model or Preprocessor file not found. Train the model first.")

# Define Input Schema
class InputFeatures(BaseModel):
    Hours_Studied: int
    Attendance: int
    Parental_Involvement: str
    Access_to_Resources: str
    Extracurricular_Activities: str
    Sleep_Hours: int
    Previous_Scores: int
    Motivation_Level: str
    Internet_Access: str
    Tutoring_Sessions: int
    Family_Income: str
    Teacher_Quality: str
    School_Type: str
    Peer_Influence: str
    Physical_Activity: int
    Learning_Disabilities: str
    Parental_Education_Level: str
    Distance_from_Home: str
    Gender: str

@app.get("/")
def home():
    return {"message": "MLOps FastAPI Model Deployed Successfully!"}

@app.post("/predict")
def predict(data: InputFeatures):
    try:
        # Convert input data to DataFrame
        input_data = pd.DataFrame([data.dict()])

        # Transform input using the preprocessor
        transformed_input = preprocessor.transform(input_data)

        # Make prediction
        prediction = model.predict(transformed_input)[0]

        return {"predicted_score": float(prediction)}

    except Exception as e:
        return {"error": str(e)}
3ï¸âƒ£ Create a requirements.txt for FastAPI
Add the following dependencies in requirements.txt:


fastapi
uvicorn
scikit-learn
numpy
pandas
pickle5
4ï¸âƒ£ Create a Dockerfile
This Dockerfile containerizes your FastAPI application.

Create Dockerfile in the root folder
dockerfile
# Use Python base image
FROM python:3.9-slim

# Set working directory
WORKDIR /app

# Copy required files
COPY requirements.txt .
COPY src /app/src
COPY artifacts /app/artifacts

# Install dependencies
RUN pip install --no-cache-dir -r requirements.txt

# Expose port
EXPOSE 8000

# Run FastAPI app
CMD ["uvicorn", "src.app:app", "--host", "0.0.0.0", "--port", "8000"]
5ï¸âƒ£ Build & Run the FastAPI App Locally
Run FastAPI Without Docker
cd MLOPS
uvicorn src.app:app --host 0.0.0.0 --port 8000 --reload
Visit http://127.0.0.1:8000/docs to test the API.
6ï¸âƒ£ Build & Run Docker Container
Build the Docker Image
docker build -t mlops-fastapi .
Run the Docker Container
docker run -p 8000:8000 mlops-fastapi
The FastAPI app will be running at http://localhost:8000.
Test the API using Postman or cURL.
7ï¸âƒ£ Push the Docker Image to Docker Hub
Login to Docker Hub
docker login

Tag the Image
docker tag mlops-fastapi YOUR_DOCKERHUB_USERNAME/mlops-fastapi
Push the Image

docker push YOUR_DOCKERHUB_USERNAME/mlops-fastapi
8ï¸âƒ£ Deploy FastAPI on Cloud (Optional)
To deploy your container to AWS, Azure, or GCP, you can:

Use AWS Elastic Beanstalk (eb init, eb deploy).
Deploy using Google Cloud Run.
Use Azure Container Apps.
ğŸ¯ Summary of What We Did
Step	Command
Save model & preprocessor	save_model_and_preprocessor(model, preprocessor)
Run FastAPI locally	uvicorn src.app:app --host 0.0.0.0 --port 8000 --reload
Build Docker image	docker build -t mlops-fastapi .
Run Docker container	docker run -p 8000:8000 mlops-fastapi
Push Docker image	docker push YOUR_DOCKERHUB_USERNAME/mlops-fastapi
Now your MLOps FastAPI model is containerized and ready for deployment! ğŸš€ Let me know if you need any modifications. ğŸ˜Š

the updated project structure with the necessary files for FastAPI deployment and Dockerization:
MLOPS/
â”‚â”€â”€ artifacts/               # Stores trained models and preprocessing pipelines
â”‚   â”œâ”€â”€ best_model.pkl       # Trained best model
â”‚   â”œâ”€â”€ preprocessor.pkl     # Preprocessing pipeline
â”‚
â”‚â”€â”€ catboost_info/           # CatBoost internal logs (if using CatBoost)
â”‚â”€â”€ Logs/                    # Stores logs for monitoring/debugging
â”‚â”€â”€ mlproject.egg-info/      # Python package metadata
â”‚â”€â”€ notebook/                # Jupyter Notebooks for EDA & experimentation
â”‚
â”‚â”€â”€ src/                     # Source code directory
â”‚   â”‚â”€â”€ components/          # Core ML components
â”‚   â”‚   â”œâ”€â”€ data_injestion.py      # Handles data loading and train-test split
â”‚   â”‚   â”œâ”€â”€ data_transformation.py # Preprocessing and feature transformation logic
â”‚   â”‚   â”œâ”€â”€ model_trainer.py       # Model training and evaluation logic
â”‚   â”‚
â”‚   â”‚â”€â”€ pipeline/            # Training and prediction pipelines
â”‚   â”‚   â”œâ”€â”€ train_pipeline.py      # Code to execute the training pipeline
â”‚   â”‚   â”œâ”€â”€ predict_pipeline.py    # Code for inference on new data
â”‚   â”‚   â”œâ”€â”€ utils.py               # Utility functions (saving/loading models, evaluation, etc.)
â”‚   â”‚
â”‚   â”‚â”€â”€ app.py               # FastAPI service for model deployment
â”‚   â”‚â”€â”€ logger.py            # Logger configuration
â”‚   â”‚â”€â”€ exception.py         # Custom exception handling
â”‚
â”‚â”€â”€ venv/                    # Python virtual environment (should be in .gitignore)
â”‚â”€â”€ .gitignore               # Ignore unnecessary files
â”‚â”€â”€ README.md                # Project documentation
â”‚â”€â”€ requirements.txt         # Python dependencies
â”‚â”€â”€ Dockerfile               # Docker configuration for FastAPI app
â”‚â”€â”€ setup.py                 # Python package setup file
ğŸ’¡ Key Additions
artifacts/:

best_model.pkl (Trained Model)
preprocessor.pkl (Feature transformation pipeline)
src/app.py:

FastAPI app for model inference.
Loads best_model.pkl and preprocessor.pkl.
Exposes /predict API endpoint.
Dockerfile:

Defines a containerized environment for deployment.
Uses Uvicorn to run FastAPI.
requirements.txt:

Lists dependencies (fastapi, scikit-learn, pandas, uvicorn, etc.)
ğŸš€ Next Steps
Train and save your best model & preprocessor in artifacts/.
Run FastAPI locally using:
uvicorn src.app:app --host 0.0.0.0 --port 8000 --reload
Containerize and deploy:
Build Docker image:
docker build -t mlops-fastapi .
Run container:
docker run -p 8000:8000 mlops-fastapi
Deploy to Docker Hub, AWS, or GCP.
